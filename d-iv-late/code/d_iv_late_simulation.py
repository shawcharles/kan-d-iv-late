# -*- coding: utf-8 -*-
"""D-IV-LATE_simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wxNgwjoojees20lb29Ni4vAdQ-QKlTyK
"""

# This file will contain the code for our simulation study.
# We will implement the D-LATE estimator and test its performance.

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import KFold

# --- 1. Data Generating Process (DGP) ---
def generate_dlate_data(n_samples=2000, n_features=5, seed=42):
    """
    Generates a dataset with a known D-LATE.
    - Z: Instrument
    - X: Covariates
    - W: Endogenous treatment
    - Y: Outcome
    """
    np.random.seed(seed)

    # Generate covariates
    X = np.random.randn(n_samples, n_features)

    # Generate instrument
    Z = np.random.binomial(1, 0.5, n_samples)

    # Define the "complier" status (unobserved)
    # Let's say 50% of the population are compliers
    is_complier = np.random.binomial(1, 0.5, n_samples)

    # Treatment assignment (endogenous)
    # Compliers' treatment depends on Z. Non-compliers are always-takers or never-takers.
    # This setup creates endogeneity because the error term in Y will be correlated with is_complier.
    W = np.zeros(n_samples)
    W[is_complier == 1] = Z[is_complier == 1]
    W[is_complier == 0] = np.random.binomial(1, 0.5, np.sum(is_complier == 0)) # Always/never takers

    # Potential outcomes
    # Let the treatment effect be heterogeneous and depend on an unobserved factor correlated with complier status
    base_outcome = 10 + X[:, 0] + np.random.randn(n_samples)

    # For compliers, the treatment has a significant effect on the distribution
    # It shifts the mean and reduces the variance
    Y0_complier = base_outcome + 2 * X[:, 1]
    Y1_complier = base_outcome + 10 + 1 * X[:, 1]

    # For non-compliers, the treatment has no effect
    Y0_non_complier = base_outcome
    Y1_non_complier = base_outcome

    # Generate observed outcome Y
    Y = np.zeros(n_samples)

    # For compliers
    Y[(is_complier == 1) & (W == 0)] = Y0_complier[(is_complier == 1) & (W == 0)]
    Y[(is_complier == 1) & (W == 1)] = Y1_complier[(is_complier == 1) & (W == 1)]

    # For non-compliers
    Y[(is_complier == 0) & (W == 0)] = Y0_non_complier[(is_complier == 0) & (W == 0)]
    Y[(is_complier == 0) & (W == 1)] = Y1_non_complier[(is_complier == 0) & (W == 1)]

    # True D-LATE can be computed from Y0_complier and Y1_complier
    # For a given y, D-LATE(y) = P(Y1_complier <= y) - P(Y0_complier <= y)

    data = pd.DataFrame(X, columns=[f'X{i}' for i in range(n_features)])
    data['Z'] = Z
    data['W'] = W
    data['Y'] = Y

    return data, (Y0_complier, Y1_complier)

# --- 2. Nuisance Function Estimators ---
def estimate_nuisance_functions(data, y_grid, k_folds=5):
    """
    Estimates the three nuisance functions using K-fold cross-fitting.
    - mu(y, w, x): E[1{Y<=y} | W=w, X=x]
    - p(z, x): P(W=1 | Z=z, X=x)
    - pi(x): P(Z=1 | X=x)

    Returns a dataframe with out-of-sample predictions for each nuisance function.
    """
    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

    # Initialize empty arrays to store out-of-sample predictions
    mu_hat = np.zeros((len(data), len(y_grid)))
    p_hat = np.zeros(len(data))
    pi_hat = np.zeros(len(data))

    X_cols = [col for col in data.columns if col.startswith('X')]

    for train_index, test_index in kf.split(data):
        data_train, data_test = data.iloc[train_index], data.iloc[test_index]

        # Estimate pi(x) = P(Z=1 | X)
        pi_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=42)
        pi_model.fit(data_train[X_cols], data_train['Z'])
        pi_hat[test_index] = pi_model.predict_proba(data_test[X_cols])[:, 1]

        # Estimate p(z, x) = P(W=1 | Z, X)
        p_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=42)
        p_model.fit(data_train[X_cols + ['Z']], data_train['W'])
        p_hat[test_index] = p_model.predict_proba(data_test[X_cols + ['Z']])[:, 1]

        # Estimate mu(y, w, x) = E[1{Y<=y} | W, X] for each y in y_grid
        for i, y_val in enumerate(y_grid):
            data_train_y = data_train.copy()
            data_train_y['Y_le_y'] = (data_train_y['Y'] <= y_val).astype(int)

            # Check for single class in target, which causes predict_proba to fail.
            if len(data_train_y['Y_le_y'].unique()) == 1:
                mu_hat[test_index, i] = data_train_y['Y_le_y'].iloc[0]
            else:
                mu_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=42)
                mu_model.fit(data_train_y[X_cols + ['W']], data_train_y['Y_le_y'])
                mu_hat[test_index, i] = mu_model.predict_proba(data_test[X_cols + ['W']])[:, 1]

    # Store nuisance predictions in a new dataframe
    nuisance_df = pd.DataFrame({
        'pi_hat': pi_hat,
        'p_hat': p_hat,
    })
    for i, y_val in enumerate(y_grid):
        nuisance_df[f'mu_hat_{y_val}'] = mu_hat[:, i]

    return nuisance_df

# --- 3. D-LATE Estimator ---
def dlate_estimator(data, nuisance_df, y_grid):
    """
    Computes the D-LATE using the doubly-robust scores.
    """
    Z = data['Z'].values
    W = data['W'].values
    Y = data['Y'].values

    pi_hat = nuisance_df['pi_hat'].values
    p_hat = nuisance_df['p_hat'].values

    # Estimate mu_hat(y, 1, x) and mu_hat(y, 0, x)
    # This requires fitting models on the whole dataset, which is a simplification for this simulation.
    # A full implementation would use the cross-fitting scheme properly.
    X_cols = [col for col in data.columns if col.startswith('X')]
    mu_hat_1 = np.zeros((len(data), len(y_grid)))
    mu_hat_0 = np.zeros((len(data), len(y_grid)))

    for i, y_val in enumerate(y_grid):
        data_y = data.copy()
        data_y['Y_le_y'] = (data_y['Y'] <= y_val).astype(int)

        # Model for W=1
        data_train1 = data_y[data_y['W'] == 1]
        if len(data_train1) == 0:
             mu_hat_1[:, i] = 0 # No data, predict 0
        elif len(data_train1['Y_le_y'].unique()) == 1:
            mu_hat_1[:, i] = data_train1['Y_le_y'].iloc[0]
        else:
            model1 = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=42)
            model1.fit(data_train1[X_cols], data_train1['Y_le_y'])
            mu_hat_1[:, i] = model1.predict_proba(data[X_cols])[:, 1]

        # Model for W=0
        data_train0 = data_y[data_y['W'] == 0]
        if len(data_train0) == 0:
            mu_hat_0[:, i] = 0
        elif len(data_train0['Y_le_y'].unique()) == 1:
            mu_hat_0[:, i] = data_train0['Y_le_y'].iloc[0]
        else:
            model0 = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=42)
            model0.fit(data_train0[X_cols], data_train0['Y_le_y'])
            mu_hat_0[:, i] = model0.predict_proba(data[X_cols])[:, 1]

    # Compute the scores
    psi_beta = (Z - pi_hat) / (pi_hat * (1 - pi_hat)) * (W - p_hat)

    dlate_estimates = []
    for i, y_val in enumerate(y_grid):
        mu_hat_y = nuisance_df[f'mu_hat_{y_val}'].values
        alpha_hat_i = mu_hat_1[:, i] - mu_hat_0[:, i]

        psi_alpha = (Z - pi_hat) / (pi_hat * (1 - pi_hat)) * ((Y <= y_val).astype(int) - mu_hat_y) + alpha_hat_i

        # Estimate D-LATE for this y
        dlate_y = np.mean(psi_alpha) / np.mean(psi_beta)
        dlate_estimates.append(dlate_y)

    return np.array(dlate_estimates)

# --- 4. Simulation Loop ---
def run_simulation(n_simulations=100, n_samples=2000):
    """
    Runs a Monte Carlo simulation to evaluate the D-LATE estimator.
    """
    all_results = []

    for i in range(n_simulations):
        print(f"Running simulation {i+1}/{n_simulations}...")

        # 1. Generate data
        data, (Y0_complier, Y1_complier) = generate_dlate_data(n_samples=n_samples, seed=i)

        # 2. Define the grid of y values to evaluate the D-LATE on
        y_grid = np.linspace(np.min(Y0_complier), np.max(Y1_complier), 20)

        # 3. Estimate nuisance functions
        nuisance_df = estimate_nuisance_functions(data, y_grid)

        # 4. Estimate D-LATE
        dlate_est = dlate_estimator(data, nuisance_df, y_grid)

        # 5. Calculate true D-LATE
        true_dlate = np.array([np.mean(Y1_complier <= y) - np.mean(Y0_complier <= y) for y in y_grid])

        # Store results
        results = pd.DataFrame({
            'y': y_grid,
            'dlate_est': dlate_est,
            'true_dlate': true_dlate,
            'bias': dlate_est - true_dlate
        })
        all_results.append(results)

    return pd.concat(all_results)

# --- 5. Main Execution Block ---
if __name__ == "__main__":
    print("Starting simulation study for D-LATE estimator...")

    simulation_results = run_simulation(n_simulations=50, n_samples=2000) # Using fewer sims for speed

    # Calculate and print summary statistics
    avg_bias = simulation_results.groupby('y')['bias'].mean()
    rmse = np.sqrt(simulation_results.groupby('y')['bias'].apply(lambda x: np.mean(x**2)))

    print("\n--- Simulation Results ---")
    print("Average Bias across simulations:")
    print(avg_bias)
    print("\nRMSE across simulations:")
    print(rmse)

    # --- 6. Save Results ---
    results_df = pd.DataFrame({
        'y': avg_bias.index,
        'avg_bias': avg_bias.values,
        'rmse': rmse.values
    })
    results_df.to_csv('simulation_results.csv', index=False)
    print("\nResults saved to D-LATE/simulation_results.csv")
    print("\n--- End of Simulation ---")