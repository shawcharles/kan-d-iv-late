# System Patterns: D-IV-LATE Estimation Framework

## Core Architecture: Double/Debiased Machine Learning (DML)

The entire estimation strategy is built upon the Double/Debiased Machine Learning (DML) framework from Chernozhukov et al. (2018). This architecture is chosen for its theoretical robustness, specifically its ability to produce `sqrt(n)`-consistent estimates of a target parameter (D-IV-LATE) even when using flexible, high-dimensional machine learning models for nuisance components.

The key pattern is the use of **Neyman-orthogonal moments**. The estimating equations are constructed in a way that makes them insensitive to first-order errors in the estimation of the nuisance functions. This is the core principle that allows for the use of regularized models like KANs or Random Forests without introducing first-order bias into the final D-IV-LATE estimate.

## Key Pattern: Cross-Fitting (K-Fold)

To prevent overfitting bias and to satisfy the theoretical requirements of the DML framework, a **K-fold cross-fitting** procedure is implemented as a core system pattern.

The workflow is as follows:
1.  The dataset is split into `K` disjoint folds.
2.  For each fold `k`, the nuisance functions are trained on the data from all other folds (`K-1` folds).
3.  The trained models are then used to predict the nuisance values for the observations *only within* fold `k`.
4.  This process is repeated for all `K` folds, ensuring that the predictions for every observation are generated by a model that was not trained on that observation.
5.  The final D-IV-LATE estimate is computed by averaging the influence function scores calculated across all observations.

This pattern is consistently applied across all simulations and the empirical application, as implemented in `kan_utils.py`. The number of folds, `K_FOLDS`, is a global hyperparameter.

## Nuisance Function Estimation as a Modular Component

The estimation of the three key nuisance functions is treated as a modular and swappable component within the system. The core logic in `kan_d_iv_late_simulation_enhanced.py` can call different modeling strategies.

The three nuisance functions are:
1.  `pi_hat = E[Z | X]`: The instrument propensity score.
2.  `p_hat_z = E[W | Z=z, X]`: The conditional treatment probability (first stage).
3.  `mu_hat_z = E[1(Y<=y) | Z=z, X]`: The conditional outcome CDF.

The `estimate_nuisance_functions_enhanced` function implements the logic for estimating these using either 'kan' or 'rf' as the `model_type`. This modularity is what enables the central comparison of the paper.

## Uncertainty Quantification Patterns

Two distinct patterns are used for estimating uncertainty:

1.  **Asymptotic Standard Errors**: Implemented in `dlate_point_se` in `kan_utils.py`. This method relies on the influence function representation from DML theory to calculate the asymptotic variance. This is computationally fast but, as initial results suggest, may provide poor coverage for the KAN estimator.
2.  **Bootstrap Confidence Intervals**: Implemented in `bootstrap_dlate_ci`. This method involves resampling the original data with replacement and re-running the entire estimation procedure many times to build an empirical distribution of the estimator. It is more computationally intensive but is often more robust, especially in finite samples or when asymptotic approximations are poor.

The `kan_performance_investigation.ipynb` workbook is designed to explicitly compare these two patterns and determine the most reliable one for the KAN estimator.
