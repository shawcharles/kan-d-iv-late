# -*- coding: utf-8 -*-
"""kan_simulation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xvlihVPS8DiTUVOV_l_nBSrF6Itl4Luu
"""

#!pip install pykan

"""# Main"""

import numpy as np
import pandas as pd
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
import os
import shutil
import zipfile
import torch
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

# Import standardized utilities and parameters
from kan_utils import (
    train_standardized_kan,
    K_FOLDS,
    DEVICE,
    MIN_CLASS_COUNT_THRESHOLD,
    dlate_point_se
)

# --- 1. Data Generating Process (DGP) ---
def generate_dlate_data(n_samples=2000, n_features=5, seed=42):
    """
    Generates a dataset with a known D-LATE and complex non-linearities.
    """
    np.random.seed(seed)

    # Generate covariates
    X = np.random.randn(n_samples, n_features)

    # Nuisance functions
    pi_x = torch.sigmoid(torch.tensor(X[:, 0] + np.sin(np.pi * X[:, 1]) - np.cos(np.pi * X[:, 2]) + 0.5 * X[:, 3]**2 - 0.5 * X[:, 4]**2)).numpy()
    Z = np.random.binomial(1, pi_x)

    p_zx = torch.sigmoid(torch.tensor(Z + X[:, 0] + np.sin(np.pi * X[:, 1]) - np.cos(np.pi * X[:, 2]) + 0.5 * X[:, 3]**2 - 0.5 * X[:, 4]**2)).numpy()
    W = np.random.binomial(1, p_zx)

    # Potential outcomes
    Y0 = X[:, 0] + np.sin(np.pi * X[:, 1]) + np.random.randn(n_samples)
    Y1 = Y0 + 10 + np.cos(np.pi * X[:, 2]) + np.random.randn(n_samples)

    # Observed outcome
    Y = W * Y1 + (1 - W) * Y0

    data = pd.DataFrame(X, columns=[f'X{i}' for i in range(n_features)])
    data['Z'] = Z
    data['W'] = W
    data['Y'] = Y

    # True D-LATE for compliers (in this DGP, all are compliers)
    true_dlate_func = lambda y: np.mean(Y1 <= y) - np.mean(Y0 <= y)

    return data, true_dlate_func

# --- 2. Nuisance Function Estimators ---
def estimate_nuisance_functions(data, y_grid, model_type='kan'):
    """
    Estimates nuisance functions using K-fold cross-fitting and generates
    counterfactual predictions required for the DML LATE estimator.
    """
    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42)

    pi_hat = np.zeros(len(data))
    p_hat_0 = np.zeros(len(data))
    p_hat_1 = np.zeros(len(data))
    mu_hat_0 = np.zeros((len(data), len(y_grid)))
    mu_hat_1 = np.zeros((len(data), len(y_grid)))

    X_cols = [col for col in data.columns if col.startswith('X')]

    for train_index, test_index in tqdm(kf.split(data), total=K_FOLDS, desc=f'Cross-fitting {model_type}'):
        data_train, data_test = data.iloc[train_index], data.iloc[test_index]

        # --- Feature Scaling ---
        scaler_X = StandardScaler().fit(data_train[X_cols])
        X_train_scaled = scaler_X.transform(data_train[X_cols])
        X_test_scaled = scaler_X.transform(data_test[X_cols])

        # For models conditional on Z
        scaler_XZ = StandardScaler().fit(data_train[X_cols + ['Z']])
        XZ_train_scaled = scaler_XZ.transform(data_train[X_cols + ['Z']])
        
        X_test_z0 = np.hstack([data_test[X_cols], np.zeros((len(data_test), 1))])
        X_test_z1 = np.hstack([data_test[X_cols], np.ones((len(data_test), 1))])
        XZ_test_scaled_z0 = scaler_XZ.transform(X_test_z0)
        XZ_test_scaled_z1 = scaler_XZ.transform(X_test_z1)

        if model_type == 'kan':
            # 1. Estimate pi(x) = P(Z=1 | X)
            pi_model = train_standardized_kan(X_train_scaled, data_train['Z'].values, loss_fn='bce')
            with torch.no_grad():
                pi_hat[test_index] = torch.sigmoid(pi_model(torch.from_numpy(X_test_scaled).float().to(DEVICE))).cpu().numpy().flatten()

            # 2. Estimate p(z, x) = P(W=1 | Z, X)
            p_model = train_standardized_kan(XZ_train_scaled, data_train['W'].values, loss_fn='bce')
            with torch.no_grad():
                p_hat_0[test_index] = torch.sigmoid(p_model(torch.from_numpy(XZ_test_scaled_z0).float().to(DEVICE))).cpu().numpy().flatten()
                p_hat_1[test_index] = torch.sigmoid(p_model(torch.from_numpy(XZ_test_scaled_z1).float().to(DEVICE))).cpu().numpy().flatten()

            # 3. Estimate mu(y, z, x) = E[1{Y<=y} | Z, X]
            for i, y_val in enumerate(y_grid):
                y_train_mu = (data_train['Y'] <= y_val).astype(int).values
                if len(np.unique(y_train_mu)) < 2 or np.min(np.bincount(y_train_mu)) < MIN_CLASS_COUNT_THRESHOLD:
                    val = np.mean(y_train_mu)
                    mu_hat_0[test_index, i], mu_hat_1[test_index, i] = val, val
                else:
                    mu_model = train_standardized_kan(XZ_train_scaled, y_train_mu, loss_fn='bce')
                    with torch.no_grad():
                        mu_hat_0[test_index, i] = torch.sigmoid(mu_model(torch.from_numpy(XZ_test_scaled_z0).float().to(DEVICE))).cpu().numpy().flatten()
                        mu_hat_1[test_index, i] = torch.sigmoid(mu_model(torch.from_numpy(XZ_test_scaled_z1).float().to(DEVICE))).cpu().numpy().flatten()

        elif model_type == 'rf':
            pi_model_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1).fit(data_train[X_cols], data_train['Z'])
            pi_hat[test_index] = pi_model_rf.predict_proba(data_test[X_cols])[:, 1]

            p_model_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1).fit(data_train[X_cols + ['Z']], data_train['W'])
            p_hat_0[test_index] = p_model_rf.predict_proba(X_test_z0)[:, 1]
            p_hat_1[test_index] = p_model_rf.predict_proba(X_test_z1)[:, 1]

            for i, y_val in enumerate(y_grid):
                y_train_mu = (data_train['Y'] <= y_val).astype(int).values
                if len(np.unique(y_train_mu)) < 2:
                    mu_hat_0[test_index, i], mu_hat_1[test_index, i] = np.mean(y_train_mu), np.mean(y_train_mu)
                else:
                    mu_model_rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1).fit(data_train[X_cols + ['Z']], y_train_mu)
                    mu_hat_0[test_index, i] = mu_model_rf.predict_proba(X_test_z0)[:, 1]
                    mu_hat_1[test_index, i] = mu_model_rf.predict_proba(X_test_z1)[:, 1]

    epsilon = 1e-6
    nuisance_df = pd.DataFrame({
        'pi_hat': np.clip(pi_hat, epsilon, 1 - epsilon),
        'p_hat_0': np.clip(p_hat_0, epsilon, 1 - epsilon),
        'p_hat_1': np.clip(p_hat_1, epsilon, 1 - epsilon)
    })
    for i, y_val in enumerate(y_grid):
        nuisance_df[f'mu_hat_0_{y_val}'] = np.clip(mu_hat_0[:, i], epsilon, 1 - epsilon)
        nuisance_df[f'mu_hat_1_{y_val}'] = np.clip(mu_hat_1[:, i], epsilon, 1 - epsilon)

    return nuisance_df

# --- 3. D-LATE Estimator ---
def dlate_estimator(data, nuisance_df, y_grid):
    """
    Computes the D-LATE using the Double/Debiased Machine Learning (DML)
    influence function for the Local Average Treatment Effect (LATE).
    """
    Z, W, Y = data['Z'].values, data['W'].values, data['Y'].values
    pi_hat, p_hat_0, p_hat_1 = nuisance_df['pi_hat'].values, nuisance_df['p_hat_0'].values, nuisance_df['p_hat_1'].values

    psi_beta = (p_hat_1 - p_hat_0) + (Z / pi_hat) * (W - p_hat_1) - ((1 - Z) / (1 - pi_hat)) * (W - p_hat_0)
    E_psi_beta = np.mean(psi_beta)

    if np.abs(E_psi_beta) < 1e-8:
        return np.zeros(len(y_grid))

    dlate_estimates = []
    for i, y_val in enumerate(y_grid):
        mu_hat_0_y = nuisance_df[f'mu_hat_0_{y_val}'].values
        mu_hat_1_y = nuisance_df[f'mu_hat_1_{y_val}'].values
        y_indicator = (Y <= y_val).astype(int)

        psi_alpha = (mu_hat_1_y - mu_hat_0_y) + \
                    (Z / pi_hat) * (y_indicator - mu_hat_1_y) - \
                    ((1 - Z) / (1 - pi_hat)) * (y_indicator - mu_hat_0_y)
        
        E_psi_alpha = np.mean(psi_alpha)
        dlate_estimates.append(E_psi_alpha / E_psi_beta)

    return np.array(dlate_estimates)

# --- 4. Main Simulation Loop ---
def run_simulation(n_replications=200, model_type='kan'):
    """
    Runs the full simulation for a given number of replications.
    """
    all_results = []
    true_dlates = []
    y_grids = []
    for i in tqdm(range(n_replications), desc=f'Running Simulation ({model_type})'):
        data, true_dlate_func = generate_dlate_data(n_samples=2000, seed=i)
        y_grid = np.linspace(data['Y'].min(), data['Y'].max(), 10) 
        nuisance_df = estimate_nuisance_functions(data, y_grid, model_type=model_type)
        inf_dict = dlate_point_se(data, nuisance_df, y_grid)
        dlate_est = inf_dict['dlate']
        se = inf_dict['se']
        ci_l = inf_dict['ci_lower']
        ci_u = inf_dict['ci_upper']
        p_vals = inf_dict['p_values']
        true_dlate = np.array([true_dlate_func(y) for y in y_grid])
        results = pd.DataFrame({
            'y': y_grid,
            'dlate_est': dlate_est,
            'se': se,
            'ci_lower': ci_l,
            'ci_upper': ci_u,
            'p_value': p_vals,
            'true_dlate': true_dlate,
            'bias': dlate_est - true_dlate
        })
        all_results.append(results)
        true_dlates.append(true_dlate)
        y_grids.append(y_grid)
    return pd.concat(all_results), np.array(true_dlates), np.array(y_grids)

# --- 5. Execution ---
if __name__ == '__main__':
    # Set up logging to capture output even if console output is not visible
    import sys
    import datetime
    
    log_file = '../results/simulation_log.txt'
    os.makedirs('../results', exist_ok=True)
    
    class Tee:
        def __init__(self, *files):
            self.files = files
        def write(self, obj):
            for f in self.files:
                f.write(obj)
                f.flush()
        def flush(self):
            for f in self.files:
                f.flush()
    
    with open(log_file, 'w') as f:
        f.write(f"Simulation started at {datetime.datetime.now()}\n")
        f.write("=" * 50 + "\n")
        
        # Redirect stdout to both console and file
        original_stdout = sys.stdout
        sys.stdout = Tee(sys.stdout, f)
    # Run for KAN
    print("Running simulation for KAN model...")
    kan_results_df, kan_true_dlate, y_grid = run_simulation(model_type='kan')

    print("\n--- KAN Results DataFrame ---")
    print(kan_results_df.head())
    print(f"KAN results shape: {kan_results_df.shape}")

    results_dir = '../results'
    os.makedirs(results_dir, exist_ok=True)

    try:
        kan_path = os.path.join(results_dir, 'kan_simulation_results.csv')
        print(f"Attempting to save KAN results to: {kan_path}")
        kan_results_df.to_csv(kan_path, index=False)
        print("KAN simulation finished. Results saved successfully.")
    except Exception as e:
        print(f"ERROR: Failed to save KAN results: {e}")

    # Run for Random Forest
    print("\nRunning simulation for Random Forest model...")
    rf_results_df, _, _ = run_simulation(model_type='rf')

    print("\n--- RF Results DataFrame ---")
    print(rf_results_df.head())
    print(f"RF results shape: {rf_results_df.shape}")

    try:
        rf_path = os.path.join(results_dir, 'rf_simulation_results.csv')
        print(f"Attempting to save RF results to: {rf_path}")
        rf_results_df.to_csv(rf_path, index=False)
        print("Random Forest simulation finished. Results saved successfully.")
    except Exception as e:
        print(f"ERROR: Failed to save RF results: {e}")

    # Save true D-LATE values for plotting
    true_dlate_df = pd.DataFrame({'y': y_grid, 'true_dlate': kan_true_dlate})

    print("\n--- True D-LATE DataFrame ---")
    print(true_dlate_df.head())
    print(f"True D-LATE shape: {true_dlate_df.shape}")

    try:
        true_dlate_path = os.path.join(results_dir, 'true_dlate_values.csv')
        print(f"Attempting to save True D-LATE to: {true_dlate_path}")
        true_dlate_df.to_csv(true_dlate_path, index=False)
        print("True D-LATE values saved successfully.")
    except Exception as e:
        print(f"ERROR: Failed to save True D-LATE values: {e}")

        print(f"\nAll simulation results attempted to be saved in '{results_dir}' directory.")

        print("\n--- End of Simulation ---")
        
        # Restore original stdout
        sys.stdout = original_stdout
        
    print(f"Simulation completed. Check {log_file} for detailed output.")
